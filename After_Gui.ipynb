{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ8iZQaenWRw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac373f3c-d2aa-44e3-a8a9-2c459b3ff5ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rarfile\n",
            "  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: rarfile\n",
            "Successfully installed rarfile-4.0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.2-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.2 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Collecting simpletransformers\n",
            "  Downloading simpletransformers-0.63.11-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.7/250.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.65.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2022.10.31)\n",
            "Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.30.2)\n",
            "Collecting datasets (from simpletransformers)\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n",
            "Collecting seqeval (from simpletransformers)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.5.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.13.3)\n",
            "Collecting wandb>=0.10.32 (from simpletransformers)\n",
            "  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit (from simpletransformers)\n",
            "  Downloading streamlit-1.24.0-py2.py3-none-any.whl (8.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from simpletransformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (0.16.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (0.3.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.10.32->simpletransformers)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb>=0.10.32->simpletransformers)\n",
            "  Downloading sentry_sdk-1.27.1-py2.py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.7/211.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.10.32->simpletransformers)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb>=0.10.32->simpletransformers)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb>=0.10.32->simpletransformers)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets->simpletransformers)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets->simpletransformers)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->simpletransformers)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.8.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (3.1.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.2.2)\n",
            "Collecting blinker<2,>=1.0.0 (from streamlit->simpletransformers)\n",
            "  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.3.1)\n",
            "Collecting importlib-metadata<7,>=1.4 (from streamlit->simpletransformers)\n",
            "  Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pillow<10,>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (8.4.0)\n",
            "Collecting pympler<2,>=0.9 (from streamlit->simpletransformers)\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich<14,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (13.4.2)\n",
            "Requirement already satisfied: tenacity<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (8.2.2)\n",
            "Requirement already satisfied: toml<2 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.6.3)\n",
            "Collecting tzlocal<5,>=1.1 (from streamlit->simpletransformers)\n",
            "  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting validators<1,>=0.2 (from streamlit->simpletransformers)\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pydeck<1,>=0.1.dev5 (from streamlit->simpletransformers)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.3.1)\n",
            "Collecting watchdog (from streamlit->simpletransformers)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.56.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.40.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.10.32->simpletransformers) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit->simpletransformers) (3.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.11.0->streamlit->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.11.0->streamlit->simpletransformers) (2.14.0)\n",
            "Collecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit->simpletransformers)\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from validators<1,>=0.2->streamlit->simpletransformers) (4.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.3)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.19.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.11.0->streamlit->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (3.2.2)\n",
            "Collecting tzdata (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit->simpletransformers)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: seqeval, validators, pathtools\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=0b3ded14acbe381d5b0b6807ab14c85f69ce77057a4d2868ed53263611fddaf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=0cb7afd1462bfd489a67e7afdc387d7eafc21765f53fad1dfa74fd075d79a459\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/ed/dd/d3a556ad245ef9dc570c6bcd2f22886d17b0b408dd3bbb9ac3\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=03b1f12556679490dedc3790faed5293d9ba2c3666dd8ea5d8fe6dceb5e16e47\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built seqeval validators pathtools\n",
            "Installing collected packages: sentencepiece, pathtools, xxhash, watchdog, validators, tzdata, smmap, setproctitle, sentry-sdk, pympler, importlib-metadata, docker-pycreds, dill, blinker, pytz-deprecation-shim, pydeck, multiprocess, gitdb, tzlocal, seqeval, GitPython, wandb, streamlit, datasets, simpletransformers\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.0.1\n",
            "    Uninstalling tzlocal-5.0.1:\n",
            "      Successfully uninstalled tzlocal-5.0.1\n",
            "Successfully installed GitPython-3.1.31 blinker-1.6.2 datasets-2.13.1 dill-0.3.6 docker-pycreds-0.4.0 gitdb-4.0.10 importlib-metadata-6.7.0 multiprocess-0.70.14 pathtools-0.1.2 pydeck-0.8.1b0 pympler-1.0.1 pytz-deprecation-shim-0.1.0.post0 sentencepiece-0.1.99 sentry-sdk-1.27.1 seqeval-1.2.2 setproctitle-1.3.2 simpletransformers-0.63.11 smmap-5.0.0 streamlit-1.24.0 tzdata-2023.3 tzlocal-4.3.1 validators-0.20.0 wandb-0.15.5 watchdog-3.0.0 xxhash-3.2.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unrar is already the newest version (1:5.6.6-2build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ],
      "source": [
        "!pip install rarfile\n",
        "! pip install transformers\n",
        "! pip install simpletransformers\n",
        "!apt-get install unrar\n",
        "! pip install fuzzywuzzy\n",
        "! pip install streamlit==1.13.0 -q\n",
        "!pip install joblib\n",
        "from fuzzywuzzy import fuzz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOHW4nr6pO1S",
        "outputId": "3716b1d0-c6aa-445f-d41e-0333939a4750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir Output_Files\n",
        "!mkdir Output_Files/test_data\n",
        "\n",
        "\n",
        "# Define the path to your ZIP file\n",
        "rar_file_path  = \"/content/drive/MyDrive/test_data.rar\"\n",
        "\n",
        "# Define the path to the output directory where you want to extract the contents\n",
        "output_directory = \"/content/Output_Files\"\n",
        "\n",
        "# Run the unzip command to extract the contents of the ZIP file\n",
        "!unrar x \"{rar_file_path}\" \"{output_directory}\""
      ],
      "metadata": {
        "id": "lqCZ3ohqJdYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88f1681-4020-40dc-cea3-81893cb5f31a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UNRAR 5.61 beta 1 freeware      Copyright (c) 1993-2018 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/drive/MyDrive/test_data.rar\n",
            "\n",
            "Extracting  /content/Output_Files/test_data/MTI.csv                      \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/الجامعة الألمانية في القاهرة.csv     \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/الجامعة الأمريكية في القاهرة.csv     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/الجامعة البريطانية في مصر.csv     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/الجامعة الدولية لمصر.csv     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/الجامعة الفرنسية في مصر.csv     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/الجامعة المصرية اليابانية للعلوم والتكنولوجيا.csv     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/الكلية الكندية الدولية.csv     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة 6 أكتوبر.csv           \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة أسوان.csv              \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة أسيوط.csv              \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة الأزهر.csv             \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة الإسكندرية.csv         \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة الجلالة.csv            \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة الفيوم.csv             \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة القاهرة.csv            \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة المنصورة.csv           \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة المنوفية.csv           \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة المنيا.csv             \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة بنها.csv               \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة بني سويف.csv           \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة جنوب الوادي.csv        \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة حلوان.csv              \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة دمنهور.csv             \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة سوهاج.csv              \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة طنطا.csv               \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة عين شمس.csv            \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة قناة السويس.csv        \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/جامعة كفر الشيخ.csv          \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مركز القلب هليوبوليس.csv     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى 6 اكتوبر للتأمين الصحى.csv     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى أحمد ماهر التعليمي.csv     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى الجلاء التعليمى.csv     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى الجنزورى التخصصى.csv     \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى الحسين التخصصي.csv     \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى الدمرداش.csv          \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى الرحمة التخصصي.csv     \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى السلام الدولى المعادي.csv     \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى السلام الدولى.csv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى الفتح الإسلامى.csv     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى القاهرة التخصصي.csv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى المنيرة العام.csv     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى الميريلاند.csv        \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى النزهة الدولي.csv     \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى باب الشعرية (سيد جلال).csv     \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى حميات العباسية.csv     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى دار الفؤاد.csv        \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى رابعة العدوية التخصصى.csv     \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى شرم الشيخ الدولى.csv     \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى عين شمس التخصصي.csv     \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى غمرة العسكري.csv      \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى فلسطين.csv            \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى كليوباتر.csv          \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى لوران.csv             \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى مدينة نصر للتأمين الصحى.csv     \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى منشية البكرى العام.csv     \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفى نور الاسلام.csv       \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفي بداية.csv             \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفی 57357.csv             \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفی الامل.csv             \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفی القصر العيني.csv      \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مستشفی زايد العسكري.csv      \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار اكتوبر الجوي.csv        \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار الاسكندرية الدولي.csv     \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار الاقصر الدولي.csv       \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار الشيخ جبريل.csv         \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار العاصمة الدولي.csv      \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار القاهرة الدولي 2.csv     \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار القاهرة الدولي القديم صالة 1.csv     \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار القاهرة الدولي صالة السفر الداخلية والوصول.csv     \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار القاهرة الدولي.csv      \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار بورسعيد الدولي.csv      \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار سفنكس الدولى.csv        \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار سفنكس الدولي.csv        \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار شرم الشيخ الدولي.csv     \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار مرسي علم الدولي.csv     \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/مطار مصر القديم.csv          \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Extracting  /content/Output_Files/test_data/وزارة الصحة والسكان.csv      \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rarfile\n",
        "\n",
        "# Open the RAR file\n",
        "with rarfile.RarFile(rar_file_path, 'r') as rar_file:\n",
        "    # Print the number of files in the RAR file\n",
        "    print(len(rar_file.namelist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwJiWwCyMYz_",
        "outputId": "7340fbce-a6a4-45a3-da60-77037a92210f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the folder\n",
        "folder_path = \"/content/Output_Files/test_data\"\n",
        "\n",
        "# Use os.listdir() to get a list of all the files in the folder\n",
        "file_names = os.listdir(folder_path)\n",
        "\n",
        "# Use len() to get the number of files in the folder\n",
        "num_files = len(file_names)\n",
        "\n",
        "print(f\"There are {num_files} files in the folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYD9i3EJNCzC",
        "outputId": "2415fe40-31a1-41cf-f397-d200454cd1b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 78 files in the folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from fuzzywuzzy import fuzz\n",
        "import torch\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import joblib\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#################### Title ######################\n",
        "st.title(\"WELCOME TO OUR RECOMMENDATION SYSTEM 😍😍😍\")\n",
        "st.write(\"WE ARE HERE TO HELP YOU...........!!!\")\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "arr = os.listdir('/content/Output_Files/test_data')\n",
        "all_datasets=[]\n",
        "names=[]\n",
        "\n",
        "import streamlit as st\n",
        "st.session_state['max_value']=-10000\n",
        "rec=\" \"\n",
        "emojis = {\n",
        "    \"🙂\":\"يبتسم\",\n",
        "    \"😂\":\"يضحك\",\n",
        "    \"💔\":\"قلب حزين\",\n",
        "    \"🙂\":\"يبتسم\",\n",
        "    \"❤️\":\"حب\",\n",
        "    \"❤\":\"حب\",\n",
        "    \"😍\":\"حب\",\n",
        "    \"😭\":\"يبكي\",\n",
        "    \"😢\":\"حزن\",\n",
        "    \"😔\":\"حزن\",\n",
        "    \"♥\":\"حب\",\n",
        "    \"💜\":\"حب\",\n",
        "    \"😅\":\"يضحك\",\n",
        "    \"🙁\":\"حزين\",\n",
        "    \"💕\":\"حب\",\n",
        "    \"💙\":\"حب\",\n",
        "    \"😞\":\"حزين\",\n",
        "    \"😊\":\"سعادة\",\n",
        "    \"👏\":\"يصفق\",\n",
        "    \"👌\":\"احسنت\",\n",
        "    \"😴\":\"ينام\",\n",
        "    \"😀\":\"يضحك\",\n",
        "    \"😌\":\"حزين\",\n",
        "    \"🌹\":\"وردة\",\n",
        "    \"🙈\":\"حب\",\n",
        "    \"😄\":\"يضحك\",\n",
        "    \"😐\":\"محايد\",\n",
        "    \"✌\":\"منتصر\",\n",
        "    \"✨\":\"نجمه\",\n",
        "    \"🤔\":\"تفكير\",\n",
        "    \"😏\":\"يستهزء\",\n",
        "    \"😒\":\"يستهزء\",\n",
        "    \"🙄\":\"ملل\",\n",
        "    \"😕\":\"عصبية\",\n",
        "    \"😃\":\"يضحك\",\n",
        "    \"🌸\":\"وردة\",\n",
        "    \"😓\":\"حزن\",\n",
        "    \"💞\":\"حب\",\n",
        "    \"💗\":\"حب\",\n",
        "    \"😑\":\"منزعج\",\n",
        "    \"💭\":\"تفكير\",\n",
        "    \"😎\":\"ثقة\",\n",
        "    \"💛\":\"حب\",\n",
        "    \"😩\":\"حزين\",\n",
        "    \"💪\":\"عضلات\",\n",
        "    \"👍\":\"موافق\",\n",
        "    \"🙏🏻\":\"رجاء طلب\",\n",
        "    \"😳\":\"مصدوم\",\n",
        "    \"👏🏼\":\"تصفيق\",\n",
        "    \"🎶\":\"موسيقي\",\n",
        "    \"🌚\":\"صمت\",\n",
        "    \"💚\":\"حب\",\n",
        "    \"🙏\":\"رجاء طلب\",\n",
        "    \"💘\":\"حب\",\n",
        "    \"🍃\":\"سلام\",\n",
        "    \"☺\":\"يضحك\",\n",
        "    \"🐸\":\"ضفدع\",\n",
        "    \"😶\":\"مصدوم\",\n",
        "    \"✌️\":\"مرح\",\n",
        "    \"✋🏻\":\"توقف\",\n",
        "    \"😉\":\"غمزة\",\n",
        "    \"🌷\":\"حب\",\n",
        "    \"🙃\":\"مبتسم\",\n",
        "    \"😫\":\"حزين\",\n",
        "    \"😨\":\"مصدوم\",\n",
        "    \"🎼 \":\"موسيقي\",\n",
        "    \"🍁\":\"مرح\",\n",
        "    \"🍂\":\"مرح\",\n",
        "    \"💟\":\"حب\",\n",
        "    \"😪\":\"حزن\",\n",
        "    \"😆\":\"يضحك\",\n",
        "    \"😣\":\"استياء\",\n",
        "    \"☺️\":\"حب\",\n",
        "    \"😱\":\"كارثة\",\n",
        "    \"😁\":\"يضحك\",\n",
        "    \"😖\":\"استياء\",\n",
        "    \"🏃🏼\":\"يجري\",\n",
        "    \"😡\":\"غضب\",\n",
        "    \"🚶\":\"يسير\",\n",
        "    \"🤕\":\"مرض\",\n",
        "    \"‼️\":\"تعجب\",\n",
        "    \"🕊\":\"طائر\",\n",
        "    \"👌🏻\":\"احسنت\",\n",
        "    \"❣\":\"حب\",\n",
        "    \"🙊\":\"مصدوم\",\n",
        "    \"💃\":\"سعادة مرح\",\n",
        "    \"💃🏼\":\"سعادة مرح\",\n",
        "    \"😜\":\"مرح\",\n",
        "    \"👊\":\"ضربة\",\n",
        "    \"😟\":\"استياء\",\n",
        "    \"💖\":\"حب\",\n",
        "    \"😥\":\"حزن\",\n",
        "    \"🎻\":\"موسيقي\",\n",
        "    \"✒\":\"يكتب\",\n",
        "    \"🚶🏻\":\"يسير\",\n",
        "    \"💎\":\"الماظ\",\n",
        "    \"😷\":\"وباء مرض\",\n",
        "    \"☝\":\"واحد\",\n",
        "    \"🚬\":\"تدخين\",\n",
        "    \"💐\" : \"ورد\",\n",
        "    \"🌞\" : \"شمس\",\n",
        "    \"👆\" : \"الاول\",\n",
        "    \"⚠️\" :\"تحذير\",\n",
        "    \"🤗\" : \"احتواء\",\n",
        "    \"✖️\": \"غلط\",\n",
        "    \"📍\"  : \"مكان\",\n",
        "    \"👸\" : \"ملكه\",\n",
        "    \"👑\" : \"تاج\",\n",
        "    \"✔️\" : \"صح\",\n",
        "    \"💌\": \"قلب\",\n",
        "    \"😲\" : \"مندهش\",\n",
        "    \"💦\": \"ماء\",\n",
        "    \"🚫\" : \"خطا\",\n",
        "    \"👏🏻\" : \"برافو\",\n",
        "    \"🏊\" :\"يسبح\",\n",
        "    \"👍🏻\": \"تمام\",\n",
        "    \"⭕️\" :\"دائره كبيره\",\n",
        "    \"🎷\" : \"ساكسفون\",\n",
        "    \"👋\": \"تلويح باليد\",\n",
        "    \"✌🏼\": \"علامه النصر\",\n",
        "    \"🌝\":\"مبتسم\",\n",
        "    \"➿\"  : \"عقده مزدوجه\",\n",
        "    \"💪🏼\" : \"قوي\",\n",
        "    \"📩\":  \"تواصل معي\",\n",
        "    \"☕️\": \"قهوه\",\n",
        "    \"😧\" : \"قلق و صدمة\",\n",
        "    \"🗨\": \"رسالة\",\n",
        "    \"❗️\" :\"تعجب\",\n",
        "    \"🙆🏻\": \"اشاره موافقه\",\n",
        "    \"👯\" :\"اخوات\",\n",
        "    \"©\" :  \"رمز\",\n",
        "    \"👵🏽\" :\"سيده عجوزه\",\n",
        "    \"🐣\": \"كتكوت\",\n",
        "    \"🙌\": \"تشجيع\",\n",
        "    \"🙇\": \"شخص ينحني\",\n",
        "    \"👐🏽\":\"ايدي مفتوحه\",\n",
        "    \"👌🏽\": \"بالظبط\",\n",
        "    \"⁉️\" : \"استنكار\",\n",
        "    \"⚽️\": \"كوره\",\n",
        "    \"🕶\" :\"حب\",\n",
        "    \"🎈\" :\"بالون\",\n",
        "    \"🎀\":    \"ورده\",\n",
        "    \"💵\":  \"فلوس\",\n",
        "    \"😋\":  \"جائع\",\n",
        "    \"😛\":  \"يغيظ\",\n",
        "    \"😠\":  \"غاضب\",\n",
        "    \"✍🏻\":  \"يكتب\",\n",
        "    \"🌾\":  \"ارز\",\n",
        "    \"👣\":  \"اثر قدمين\",\n",
        "    \"❌\":\"رفض\",\n",
        "    \"🍟\":\"طعام\",\n",
        "    \"👬\":\"صداقة\",\n",
        "    \"🐰\":\"ارنب\",\n",
        "    \"☂\":\"مطر\",\n",
        "    \"⚜\":\"مملكة فرنسا\",\n",
        "    \"🐑\":\"خروف\",\n",
        "    \"🗣\":\"صوت مرتفع\",\n",
        "    \"👌🏼\":\"احسنت\",\n",
        "    \"☘\":\"مرح\",\n",
        "    \"😮\":\"صدمة\",\n",
        "    \"😦\":\"قلق\",\n",
        "    \"⭕\":\"الحق\",\n",
        "    \"✏️\":\"قلم\",\n",
        "    \"ℹ\":\"معلومات\",\n",
        "    \"🙍🏻\":\"رفض\",\n",
        "    \"⚪️\":\"نضارة نقاء\",\n",
        "    \"🐤\":\"حزن\",\n",
        "    \"💫\":\"مرح\",\n",
        "    \"💝\":\"حب\",\n",
        "    \"🍔\":\"طعام\",\n",
        "    \"❤︎\":\"حب\",\n",
        "    \"✈️\":\"سفر\",\n",
        "    \"🏃🏻‍♀️\":\"يسير\",\n",
        "    \"🍳\":\"ذكر\",\n",
        "    \"🎤\":\"مايك غناء\",\n",
        "    \"🎾\":\"كره\",\n",
        "    \"🐔\":\"دجاجة\",\n",
        "    \"🙋\":\"سؤال\",\n",
        "    \"📮\":\"بحر\",\n",
        "    \"💉\":\"دواء\",\n",
        "    \"🙏🏼\":\"رجاء طلب\",\n",
        "    \"💂🏿 \":\"حارس\",\n",
        "    \"🎬\":\"سينما\",\n",
        "    \"♦️\":\"مرح\",\n",
        "    \"💡\":\"قكرة\",\n",
        "    \"‼\":\"تعجب\",\n",
        "    \"👼\":\"طفل\",\n",
        "    \"🔑\":\"مفتاح\",\n",
        "    \"♥️\":\"حب\",\n",
        "    \"🕋\":\"كعبة\",\n",
        "    \"🐓\":\"دجاجة\",\n",
        "    \"💩\":\"معترض\",\n",
        "    \"👽\":\"فضائي\",\n",
        "    \"☔️\":\"مطر\",\n",
        "    \"🍷\":\"عصير\",\n",
        "    \"🌟\":\"نجمة\",\n",
        "    \"☁️\":\"سحب\",\n",
        "    \"👃\":\"معترض\",\n",
        "    \"🌺\":\"مرح\",\n",
        "    \"🔪\":\"سكينة\",\n",
        "    \"♨\":\"سخونية\",\n",
        "    \"👊🏼\":\"ضرب\",\n",
        "    \"✏\":\"قلم\",\n",
        "    \"🚶🏾‍♀️\":\"يسير\",\n",
        "    \"👊\":\"ضربة\",\n",
        "    \"◾️\":\"وقف\",\n",
        "    \"😚\":\"حب\",\n",
        "    \"🔸\":\"مرح\",\n",
        "    \"👎🏻\":\"لا يعجبني\",\n",
        "    \"👊🏽\":\"ضربة\",\n",
        "    \"😙\":\"حب\",\n",
        "    \"🎥\":\"تصوير\",\n",
        "    \"👉\":\"جذب انتباه\",\n",
        "    \"👏🏽\":\"يصفق\",\n",
        "    \"💪🏻\":\"عضلات\",\n",
        "    \"🏴\":\"اسود\",\n",
        "    \"🔥\":\"حريق\",\n",
        "    \"😬\":\"عدم الراحة\",\n",
        "    \"👊🏿\":\"يضرب\",\n",
        "    \"🌿\":\"ورقه شجره\",\n",
        "    \"✋🏼\":\"كف ايد\",\n",
        "    \"👐\":\"ايدي مفتوحه\",\n",
        "    \"☠️\":\"وجه مرعب\",\n",
        "    \"🎉\":\"يهنئ\",\n",
        "    \"🔕\" :\"صامت\",\n",
        "    \"😿\":\"وجه حزين\",\n",
        "    \"☹️\":\"وجه يائس\",\n",
        "    \"😘\" :\"حب\",\n",
        "    \"😰\" :\"خوف و حزن\",\n",
        "    \"🌼\":\"ورده\",\n",
        "    \"💋\":  \"بوسه\",\n",
        "    \"👇\":\"لاسفل\",\n",
        "    \"❣️\":\"حب\",\n",
        "    \"🎧\":\"سماعات\",\n",
        "    \"📝\":\"يكتب\",\n",
        "    \"😇\":\"دايخ\",\n",
        "    \"😈\":\"رعب\",\n",
        "    \"🏃\":\"يجري\",\n",
        "    \"✌🏻\":\"علامه النصر\",\n",
        "    \"🔫\":\"يضرب\",\n",
        "    \"❗️\":\"تعجب\",\n",
        "    \"👎\":\"غير موافق\",\n",
        "    \"🔐\":\"قفل\",\n",
        "    \"👈\":\"لليمين\",\n",
        "    \"™\":\"رمز\",\n",
        "    \"🚶🏽\":\"يتمشي\",\n",
        "    \"😯\":\"متفاجأ\",\n",
        "    \"✊\":\"يد مغلقه\",\n",
        "    \"😻\":\"اعجاب\",\n",
        "    \"🙉\" :\"قرد\",\n",
        "    \"👧\":\"طفله صغيره\",\n",
        "    \"🔴\":\"دائره حمراء\",\n",
        "    \"💪🏽\":\"قوه\",\n",
        "    \"💤\":\"ينام\",\n",
        "    \"👀\":\"ينظر\",\n",
        "    \"✍🏻\":\"يكتب\",\n",
        "    \"❄️\":\"تلج\",\n",
        "    \"💀\":\"رعب\",\n",
        "    \"😤\":\"وجه عابس\",\n",
        "    \"🖋\":\"قلم\",\n",
        "    \"🎩\":\"كاب\",\n",
        "    \"☕️\":\"قهوه\",\n",
        "    \"😹\":\"ضحك\",\n",
        "    \"💓\":\"حب\",\n",
        "    \"☄️ \":\"نار\",\n",
        "    \"👻\":\"رعب\",\n",
        "    \"❎\":\"خطء\",\n",
        "    \"🤮\":\"حزن\",\n",
        "    '🏻':\"احمر\"\n",
        "    }\n",
        "\n",
        "#load model\n",
        "model=joblib.load('/content/drive/MyDrive/new_model(1).pkl')\n",
        "\n",
        "##################### hendle_emojis ###############################\n",
        "def hendle_emojis(text,emojis):\n",
        "  li=[]\n",
        "  for i in word_tokenize(text):\n",
        "    for x in i :\n",
        "      if x not in emojis.keys():\n",
        "        li.append(i)\n",
        "        break\n",
        "      else:\n",
        "        li.append(emojis[x])\n",
        "  return \" \".join(li)\n",
        "####################### data cleaning ##########################\n",
        "def data_cleaning(x):\n",
        "    new = re.sub(r'[^ء-ي]',' ',x)\n",
        "    return new\n",
        "################remove_diacritics##############################\n",
        "def remove_diacritics(text):\n",
        "    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(arabic_diacritics, '', str(text))\n",
        "    return text\n",
        "################   decoding outputs   ##############################\n",
        "def decode(x):\n",
        "  if x == 0:\n",
        "    return \"Negative\"\n",
        "  elif x == 1:\n",
        "    return \"Neutral\"\n",
        "  else:\n",
        "    return \"Positive\"\n",
        "##############################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "counter=0\n",
        "num= st.number_input(\"Enter The number of inputs \")\n",
        "for i in range (int(num)):\n",
        "\n",
        "    file_name = st.text_input(\" Input Number \" +str(counter+1),\"\")\n",
        "    counter+=1\n",
        "    if file_name==\"\":\n",
        "        break\n",
        "\n",
        "    if file_name+\".csv\" not in arr:\n",
        "      max_index =0\n",
        "      max=fuzz.ratio(file_name,arr[0])\n",
        "    for i in range(len(arr)):\n",
        "      x=fuzz.ratio(file_name+\".csv\",arr[i])\n",
        "      if x > max:\n",
        "        max_index=i\n",
        "        max=x\n",
        "    file_name=arr[max_index]\n",
        "    names.append(file_name)\n",
        "\n",
        "for z in names:\n",
        "    dataset = pd.read_csv(f'/content/Output_Files/test_data/{z}')\n",
        "    dataset.columns.values[0] = \"text\"\n",
        "    dataset['label'] =0\n",
        "    #st.dataframe(dataset)\n",
        "    dataset=dataset.dropna()\n",
        "    dataset=dataset.drop_duplicates()\n",
        "    dataset['text'] = dataset['text'].apply(hendle_emojis, args=[emojis])\n",
        "    dataset['text'] = dataset['text'].apply(lambda x: data_cleaning(x))\n",
        "    dataset['text'] = dataset['text'].apply(lambda x: remove_diacritics(x))\n",
        "    dataset = dataset.dropna()\n",
        "    dataset = dataset.drop_duplicates()\n",
        "    result, model_outputs, wrong_predictions = model.eval_model(dataset)\n",
        "\n",
        "    # Get the index of the maximum value for each row\n",
        "\n",
        "    max_index_per_row = np.argmax(model_outputs, axis=1)\n",
        "    decoded_labels = [decode(x) for x in max_index_per_row ]\n",
        "\n",
        "\n",
        "# Print the decoded labels\n",
        "    data = [[text, label] for text, label in zip(dataset['text'], decoded_labels)]\n",
        "    new_df = pd.DataFrame(data, columns=['text', 'label'])\n",
        "    all_datasets.append(new_df)\n",
        "\n",
        "    #Count the number of positive, negative, and neutral labels\n",
        "    pos_count = decoded_labels.count('Positive')\n",
        "    neg_count = decoded_labels.count('Negative')\n",
        "    neu_count = decoded_labels.count('Neutral')\n",
        "\n",
        "    #Visualize the counts using Streamlit and Plotly\n",
        "    import streamlit as st\n",
        "    import plotly.graph_objects as go\n",
        "\n",
        "    st.write(\"Info about:\", re.sub(r'[^ء-ي ]', ' ', z))\n",
        "    st.write('Number of Positive Labels:', pos_count)\n",
        "    st.write('Number of Negative Labels:', neg_count)\n",
        "    st.write('Number of Neutral Labels:', neu_count)\n",
        "\n",
        "    labels = ['Positive', 'Negative', 'Neutral']\n",
        "    counts = [pos_count, neg_count, neu_count]\n",
        "\n",
        "    fig = go.Figure(data=[go.Pie(labels=labels, values=counts)])\n",
        "\n",
        "# Adjust the height and width of the plot\n",
        "    fig.update_layout(\n",
        "    height=500,  # Adjust the height to your desired size\n",
        "    width=500,  # Adjust the width to your desired size\n",
        "    )\n",
        "\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    if (pos_count /neg_count) > st.session_state['max_value']:\n",
        "       rec = re.sub(r'[^ء-ي ]', ' ', z)\n",
        "       st.session_state['max_value'] = (pos_count / neg_count)\n",
        "\n",
        "\n",
        "\n",
        "st.write(\"HERE'S THE BEST THING FOR YOU 😍👉\", f\"{rec} \\U0001F48E\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb6lnk6IIGqH",
        "outputId": "ec05ec10-79f5-4cc6-e276-b97f18e27104"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!streamlit run /content/app.py & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTCKuI8aP3ea",
        "outputId": "7ae34daf-8ac3-42f4-91de-c0e8949f2baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.91.185.219\n",
            "2023-07-07 00:49:26.661 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n",
            "[#######...........] / extract:localtunnel: verb lock using /root/.npm/_locks/s\u001b[0m\u001b[K\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.91.185.219:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.553s\n",
            "your url is: https://giant-walls-poke.loca.lt\n",
            "/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2023-07-07 00:50:09.667932: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
            "  warnings.warn(\n",
            "2023-07-07 00:50:29.688  Converting to features started. Cache is not used.\n",
            "  0% 2/895 [00:00<02:24,  6.17it/s]\n",
            "2023-07-07 00:50:30.086  Saving features into cached file cache_dir/cached_dev_bert_128_3_2\n",
            "Running Evaluation: 100% 112/112 [00:02<00:00, 48.96it/s]\n",
            "2023-07-07 00:50:32.387 {'mcc': 0.0, 'f1_multiclass': (0.943508941623997, 0.9438080057204999), 'eval_loss': 4.528242428387914}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hzVMAfcu2Uv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}